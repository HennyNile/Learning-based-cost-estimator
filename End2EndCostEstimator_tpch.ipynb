{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 0. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.834354800Z",
     "start_time": "2023-08-30T13:09:46.679832400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import psycopg2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from itertools import chain\n",
    "from random import shuffle\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "torch.set_printoptions(precision=10, threshold=None, edgeitems=None, linewidth=None, profile=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.840355300Z",
     "start_time": "2023-08-30T13:12:04.835381200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load (1) table data from csv files, (2) table column names and (3) index names\n",
    "def prepare_dataset(prefix_path):\n",
    "    data={}\n",
    "    data[\"customer\"] = pd.read_csv(prefix_path+'/customer.csv',header=None,sep='|')\n",
    "    data[\"lineitem\"] = pd.read_csv(prefix_path+'/lineitem.csv',header=None,sep='|')\n",
    "    data[\"nation\"] = pd.read_csv(prefix_path+'/nation.csv',header=None,sep='|')\n",
    "    data[\"orders\"] = pd.read_csv(prefix_path+'/orders.csv',header=None,sep='|')\n",
    "    data[\"part\"] = pd.read_csv(prefix_path+'/part.csv',header=None,sep='|')\n",
    "    data[\"partsupp\"] = pd.read_csv(prefix_path+'/partsupp.csv',header=None,sep='|')\n",
    "    data[\"region\"] = pd.read_csv(prefix_path+'/region.csv',header=None,sep='|')\n",
    "    data[\"supplier\"] = pd.read_csv(prefix_path+'/supplier.csv',header=None,sep='|')\n",
    "    customer_column = [\n",
    "        'c_custkey',\n",
    "        'c_name',\n",
    "        'c_address',\n",
    "        'c_nationkey',\n",
    "        'c_phone',\n",
    "        'c_acctbal',\n",
    "        'c_mktsegment',\n",
    "        'c_comment'\n",
    "    ]\n",
    "    lineitem_column = [\n",
    "        'l_orderkey',\n",
    "        'l_partkey',\n",
    "        'l_suppkey',\n",
    "        'l_linenumber',\n",
    "        'l_quantity',\n",
    "        'l_extendedprice',\n",
    "        'l_discount',\n",
    "        'l_tax',\n",
    "        'l_returnflag',\n",
    "        'l_linestatus',\n",
    "        'l_shipdate',\n",
    "        'l_commitdate',\n",
    "        'l_receiptdate',\n",
    "        'l_shipinstruct',\n",
    "        'l_shipmode',\n",
    "        'l_comment'\n",
    "    ]\n",
    "    nation_column = [\n",
    "        'n_nationkey',\n",
    "        'n_name',\n",
    "        'n_regionkey',\n",
    "        'n_comment'\n",
    "    ]\n",
    "    orders_column = [\n",
    "        'o_orderkey',\n",
    "        'o_custkey',\n",
    "        'o_orderstatus',\n",
    "        'o_totalprice',\n",
    "        'o_orderdate',\n",
    "        'o_orderpriority',\n",
    "        'o_clerk',\n",
    "        'o_shippriority',\n",
    "        'o_comment'\n",
    "    ]\n",
    "    part_column = [\n",
    "        'p_partkey',\n",
    "        'p_name',\n",
    "        'p_mfgr',\n",
    "        'p_brand',\n",
    "        'p_type',\n",
    "        'p_size',\n",
    "        'p_container',\n",
    "        'p_retailprice',\n",
    "        'p_comment'\n",
    "    ]\n",
    "    partsupp_column = [\n",
    "        'ps_partkey',\n",
    "        'ps_suppkey',\n",
    "        'ps_availqty',\n",
    "        'ps_supplycost',\n",
    "        'ps_comment'\n",
    "    ]\n",
    "    region_column = [\n",
    "        'r_regionkey',\n",
    "        'r_name',\n",
    "        'r_comment'\n",
    "    ]\n",
    "    supplier_column = [\n",
    "        's_suppkey',\n",
    "        's_name',\n",
    "        's_address',\n",
    "        's_nationkey',\n",
    "        's_phone',\n",
    "        's_acctbal',\n",
    "        's_comment'\n",
    "    ]\n",
    "    data[\"customer\"].columns = customer_column\n",
    "    data[\"lineitem\"].columns = lineitem_column\n",
    "    data[\"nation\"].columns = nation_column\n",
    "    data[\"orders\"].columns = orders_column\n",
    "    data[\"part\"].columns = part_column\n",
    "    data[\"partsupp\"].columns = partsupp_column\n",
    "    data[\"region\"].columns = region_column\n",
    "    data[\"supplier\"].columns = supplier_column\n",
    "\n",
    "    column2pos = {}\n",
    "    column2pos[\"customer\"] = customer_column\n",
    "    column2pos[\"lineitem\"] = lineitem_column\n",
    "    column2pos[\"nation\"] = nation_column\n",
    "    column2pos[\"orders\"] = orders_column\n",
    "    column2pos[\"part\"] = part_column\n",
    "    column2pos[\"partsupp\"] = partsupp_column\n",
    "    column2pos[\"region\"] = region_column\n",
    "    column2pos[\"supplier\"] = supplier_column\n",
    "\n",
    "    tables = ['customer', 'lineitem', 'nation', 'orders', 'part', 'partsupp', 'region', 'supplier']\n",
    "    indexes = ['customer_pkey', 'idx_customer_nationkey','customer_c_nationkey_fkey',  # customer\n",
    "               'lineitem_pkey', 'idx_lineitem_orderkey', 'idx_lineitem_part_supp', 'idx_lineitem_shipdate', 'lineitem_l_orderkey_fkey', 'lineitem_l_partkey_l_suppkey_fkey',  # lineitem\n",
    "               'nation_pkey', 'idx_nation_regionkey', 'nation_n_regionkey_fkey'  # nation\n",
    "               'orders_pkey', 'idx_orders_custkey', 'idx_orders_orderdate', 'orders_o_custkey_fkey',  # orders           \n",
    "               'part_pkey',  # part\n",
    "               'partsupp_pkey', 'idx_partsupp_partkey', 'idx_partsupp_suppkey', 'partsupp_ps_partkey_fkey', 'partsupp_ps_suppkey_fkey',  # partsupp\n",
    "               'region_pkey',  # region\n",
    "               'supplier_pkey', 'idx_supplier_nation_key', 'supplier_s_nationkey_fkey'  # supplier\n",
    "               ]\n",
    "    indexes_id = {}\n",
    "    for idx, index in enumerate(indexes):\n",
    "        indexes_id[index] = idx + 1\n",
    "    physic_ops_id = {'Materialize':1, 'Sort':2, 'Hash':3, 'Merge Join':4, 'Bitmap Index Scan':5,\n",
    "     'Index Only Scan':6, 'BitmapAnd':7, 'Nested Loop':8, 'Aggregate':9, 'Result':10,\n",
    "     'Hash Join':11, 'Seq Scan':12, 'Bitmap Heap Scan':13, 'Index Scan':14, 'BitmapOr':15, 'Memoize': 16, 'Gather': 17}\n",
    "    strategy_id = {'Plain':1}\n",
    "    compare_ops_id = {'=':1, '>':2, '<':3, '!=':4, '~~':5, '!~~':6, '!Null': 7, '>=':8, '<=':9}\n",
    "    bool_ops_id = {'AND':1,'OR':2}\n",
    "    tables_id = {}\n",
    "    columns_id = {}\n",
    "    table_id = 1\n",
    "    column_id = 1\n",
    "    for table_name in tables:\n",
    "        tables_id[table_name] = table_id\n",
    "        table_id += 1\n",
    "        for column in column2pos[table_name]:\n",
    "            columns_id[table_name+'.'+column] = column_id\n",
    "            column_id += 1\n",
    "    return data, indexes_id, tables_id, columns_id, physic_ops_id, compare_ops_id, bool_ops_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.850356100Z",
     "start_time": "2023-08-30T13:12:04.835381200Z"
    }
   },
   "outputs": [],
   "source": [
    "# load word_vetctors which is a KeyedVectors object\n",
    "def load_dictionary(path):\n",
    "    word_vectors = KeyedVectors.load(path, mmap='r')\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.903380200Z",
     "start_time": "2023-08-30T13:12:04.843358100Z"
    }
   },
   "outputs": [],
   "source": [
    "# load min_max_column which is a dictionary\n",
    "def load_numeric_min_max(path):\n",
    "    with open(path,'r') as f:\n",
    "        min_max_column = json.loads(f.read())\n",
    "    return min_max_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.933354800Z",
     "start_time": "2023-08-30T13:12:04.868358900Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate (table) prefix based on column name\n",
    "def determine_prefix(column):\n",
    "    relation_name = column.split('.')[0]\n",
    "    column_name = column.split('.')[1]\n",
    "    # if relation_name == 'customer':\n",
    "    if relation_name == 'aka_title':\n",
    "        if column_name == 'title':\n",
    "            return 'title_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'char_name':\n",
    "        if column_name == 'name':\n",
    "            return 'name_'\n",
    "        elif column_name == 'name_pcode_nf':\n",
    "            return 'nf_'\n",
    "        elif column_name == 'surname_pcode':\n",
    "            return 'surname_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'movie_info_idx':\n",
    "        if column_name == 'info':\n",
    "            return 'info_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'title':\n",
    "        if column_name == 'title':\n",
    "            return 'title_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'role_type':\n",
    "        if column_name == 'role':\n",
    "            return 'role_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'movie_companies':\n",
    "        if column_name == 'note':\n",
    "            return 'note_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'info_type':\n",
    "        if column_name == 'info':\n",
    "            return 'info_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'company_type':\n",
    "        if column_name == 'kind':\n",
    "            return ''\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'company_name':\n",
    "        if column_name == 'name':\n",
    "            return 'cn_name_'\n",
    "        elif column_name == 'country_code':\n",
    "            return 'country_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'keyword':\n",
    "        if column_name == 'keyword':\n",
    "            return 'keyword_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "            \n",
    "    elif relation_name == 'movie_info':\n",
    "        if column_name == 'info':\n",
    "            return ''\n",
    "        elif column_name == 'note':\n",
    "            return 'note_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'name':\n",
    "        if column_name == 'gender':\n",
    "            return 'gender_'\n",
    "        elif column_name == 'name':\n",
    "            return 'name_'\n",
    "        elif column_name == 'name_pcode_cf':\n",
    "            return 'cf_'\n",
    "        elif column_name == 'name_pcode_nf':\n",
    "            return 'nf_'\n",
    "        elif column_name == 'surname_pcode':\n",
    "            return 'surname_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'aka_name':\n",
    "        if column_name == 'name':\n",
    "            return 'name_'\n",
    "        elif column_name == 'name_pcode_cf':\n",
    "            return 'cf_'\n",
    "        elif column_name == 'name_pcode_nf':\n",
    "            return 'nf_'\n",
    "        elif column_name == 'surname_pcode':\n",
    "            return 'surname_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'link_type':\n",
    "        if column_name == 'link':\n",
    "            return 'link_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'person_info':\n",
    "        if column_name == 'note':\n",
    "            return 'note_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'cast_info':\n",
    "        if column_name == 'note':\n",
    "            return 'note_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'comp_cast_type':\n",
    "        if column_name == 'kind':\n",
    "            return 'kind_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    elif relation_name == 'kind_type':\n",
    "        if column_name == 'kind':\n",
    "            return 'kind_'\n",
    "        else:\n",
    "            print (column)\n",
    "            raise\n",
    "    else:\n",
    "        print (column)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.933354800Z",
     "start_time": "2023-08-30T13:12:04.875355100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate vector representation (word vector + hash vector) for input value\n",
    "def get_representation(value):\n",
    "    if value in word_vectors:\n",
    "        embedded_result = np.array(list(word_vectors[value]))\n",
    "    else:\n",
    "        embedded_result = np.array([0.0 for _ in range(500)])\n",
    "    hash_result = np.array([0.0 for _ in range(500)])\n",
    "    for t in value:\n",
    "        hash_result[hash(t) % 500] = 1.0\n",
    "    return np.concatenate((embedded_result, hash_result), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.933354800Z",
     "start_time": "2023-08-30T13:12:04.903380200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate vector representation based on get_representation()\n",
    "def get_str_representation(value, column):\n",
    "    vec = np.array([])\n",
    "    count = 0\n",
    "    prefix = determine_prefix(column)\n",
    "    for v in value.split('%'):\n",
    "        if len(v) > 0:\n",
    "            if len(vec) == 0:\n",
    "                vec = get_representation(prefix+v)\n",
    "                count = 1\n",
    "            else:\n",
    "                new_vec = get_representation(prefix+v)\n",
    "                vec = vec + new_vec\n",
    "                count += 1\n",
    "    if count > 0:\n",
    "        vec = vec / float(count)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.985355Z",
     "start_time": "2023-08-30T13:12:04.918354800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate vector representation for condition operator\n",
    "def encode_condition_op(condition_op, relation_name, index_name):\n",
    "    # bool_operator + left_value + compare_operator + right_value\n",
    "    if condition_op == None:\n",
    "        vec = [0 for _ in range(condition_op_dim)]\n",
    "    elif condition_op['op_type'] == 'Bool':\n",
    "        idx = bool_ops_id[condition_op['operator']]\n",
    "        vec = [0 for _ in range(bool_ops_total_num)]\n",
    "        vec[idx-1] = 1\n",
    "    else:\n",
    "        operator = condition_op['operator']\n",
    "        left_value = condition_op['left_value']\n",
    "        if re.match(r'.+\\..+', left_value) == None:\n",
    "            if relation_name == None:\n",
    "                relation_name = index_name.split(left_value)[1].strip('_')\n",
    "            left_value = relation_name + '.' + left_value\n",
    "        else:\n",
    "            relation_name = left_value.split('.')[0]\n",
    "        left_value_idx = columns_id[left_value]\n",
    "        left_value_vec = [0 for _ in range(column_total_num)]\n",
    "        left_value_vec[left_value_idx-1] = 1\n",
    "        right_value = condition_op['right_value']\n",
    "        column_name = left_value.split('.')[1]\n",
    "        if re.match(r'^[a-z][a-zA-Z0-9_]*\\.[a-z][a-zA-Z0-9_]*$', right_value) != None and right_value.split('.')[0] in data:\n",
    "            operator_idx = compare_ops_id[operator]\n",
    "            operator_vec = [0 for _ in range(compare_ops_total_num)]\n",
    "            operator_vec[operator_idx-1] = 1\n",
    "            right_value_idx = columns_id[right_value]\n",
    "            right_value_vec = [0]\n",
    "            left_value_vec[right_value_idx-1] = 1\n",
    "        elif data[relation_name].dtypes[column_name] == 'int64' or data[relation_name].dtypes[column_name] == 'float64':\n",
    "            right_value = float(right_value)\n",
    "            value_max = min_max_column[relation_name][column_name]['max']\n",
    "            value_min = min_max_column[relation_name][column_name]['min']\n",
    "            right_value_vec = [(right_value - value_min) / (value_max - value_min)]\n",
    "            operator_idx = compare_ops_id[operator]\n",
    "            operator_vec = [0 for _ in range(compare_ops_total_num)]\n",
    "            operator_vec[operator_idx-1] = 1\n",
    "        elif re.match(r'^__LIKE__', right_value) != None:\n",
    "            operator_idx = compare_ops_id['~~']\n",
    "            operator_vec = [0 for _ in range(compare_ops_total_num)]\n",
    "            operator_vec[operator_idx-1] = 1\n",
    "            right_value = right_value.strip('\\'')[8:]\n",
    "            right_value_vec = get_str_representation(right_value, left_value).tolist()\n",
    "        elif re.match(r'^__NOTLIKE__', right_value) != None:\n",
    "            operator_idx = compare_ops_id['!~~']\n",
    "            operator_vec = [0 for _ in range(compare_ops_total_num)]\n",
    "            operator_vec[operator_idx-1] = 1\n",
    "            right_value = right_value.strip('\\'')[11:]\n",
    "            right_value_vec = get_str_representation(right_value, left_value).tolist()\n",
    "        elif re.match(r'^__NOTEQUAL__', right_value) != None:\n",
    "            operator_idx = compare_ops_id['!=']\n",
    "            operator_vec = [0 for _ in range(compare_ops_total_num)]\n",
    "            operator_vec[operator_idx-1] = 1\n",
    "            right_value = right_value.strip('\\'')[12:]\n",
    "            right_value_vec = get_str_representation(right_value, left_value).tolist()\n",
    "        elif re.match(r'^__ANY__', right_value) != None:\n",
    "            operator_idx = compare_ops_id['=']\n",
    "            operator_vec = [0 for _ in range(compare_ops_total_num)]\n",
    "            operator_vec[operator_idx-1] = 1\n",
    "            right_value = right_value.strip('\\'')[7:].strip('{}')\n",
    "            right_value_vec = []\n",
    "            count = 0\n",
    "            for v in right_value.split(','):\n",
    "                v = v.strip('\"').strip('\\'')\n",
    "                if len(v) > 0:\n",
    "                    count += 1\n",
    "                    vec = get_str_representation(v, left_value).tolist()\n",
    "                    if len(right_value_vec) == 0:\n",
    "                        right_value_vec = [0 for _ in vec]\n",
    "                    for idx, vv in enumerate(vec):\n",
    "                        right_value_vec[idx] += vv\n",
    "            for idx in range(len(right_value_vec)):\n",
    "                right_value_vec[idx] /= len(right_value.split(','))\n",
    "        elif right_value == 'None':\n",
    "            operator_idx = compare_ops_id['!Null']\n",
    "            operator_vec = [0 for _ in range(compare_ops_total_num)]\n",
    "            operator_vec[operator_idx-1] = 1\n",
    "            if operator == 'IS':\n",
    "                right_value_vec = [1]\n",
    "            elif operator == '!=':\n",
    "                right_value_vec = [0]\n",
    "            else:\n",
    "                print (operator)\n",
    "                raise\n",
    "        else:\n",
    "#             print (left_value, operator, right_value)\n",
    "            operator_idx = compare_ops_id[operator]\n",
    "            operator_vec = [0 for _ in range(compare_ops_total_num)]\n",
    "            operator_vec[operator_idx-1] = 1\n",
    "            right_value_vec = get_str_representation(right_value, left_value).tolist()\n",
    "        vec = [0 for _ in range(bool_ops_total_num)]\n",
    "        vec = vec + left_value_vec + operator_vec + right_value_vec\n",
    "    num_pad = condition_op_dim - len(vec)\n",
    "    result = np.pad(vec, (0, num_pad), 'constant')\n",
    "#     print 'condition op: ', result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.986355800Z",
     "start_time": "2023-08-30T13:12:04.940356900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate vector representation for condition\n",
    "def encode_condition(condition, relation_name, index_name, condition_max_num):\n",
    "    if len(condition) == 0:\n",
    "        vecs = [[0 for _ in range(condition_op_dim)]]\n",
    "    else:\n",
    "        vecs = [encode_condition_op(condition_op, relation_name, index_name) for condition_op in condition]\n",
    "    num_pad = condition_max_num - len(vecs)\n",
    "    result = np.pad(vecs, ((0, num_pad),(0,0)), 'constant')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.986355800Z",
     "start_time": "2023-08-30T13:12:04.954356200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate vector representation for samples\n",
    "def encode_sample(sample):\n",
    "    return np.array([int(i) for i in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:04.986355800Z",
     "start_time": "2023-08-30T13:12:04.969364700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bitand operation for samples\n",
    "def bitand(sample1, sample2):\n",
    "    return np.minimum(sample1, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.009354700Z",
     "start_time": "2023-08-30T13:12:04.983355300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate all vector representations for a node\n",
    "def encode_node_job(node, condition_max_num):\n",
    "    # operator + first_condition + second_condition + relation\n",
    "    extra_info_num = max(column_total_num, table_total_num, index_total_num)\n",
    "    operator_vec = np.array([0 for _ in range(physic_op_total_num)])\n",
    "\n",
    "    extra_info_vec = np.array([0 for _ in range(extra_info_num)])\n",
    "    condition1_vec = np.array([[0 for _ in range(condition_op_dim)] for _ in range(condition_max_num)])\n",
    "    condition2_vec = np.array([[0 for _ in range(condition_op_dim)] for _ in range(condition_max_num)])\n",
    "    ### Samples Starts\n",
    "    sample_vec = np.array([1 for _ in range(1000)])\n",
    "    ### Samples Ends\n",
    "    has_condition = 0\n",
    "    if node != None:\n",
    "        operator = node['node_type']\n",
    "        operator_idx = physic_ops_id[operator]\n",
    "        operator_vec[operator_idx-1] = 1\n",
    "        if operator == 'Materialize' or operator == 'BitmapAnd' or operator == 'Result':\n",
    "            pass\n",
    "        elif operator == 'Sort':\n",
    "            for key in node['sort_keys']:\n",
    "                extra_info_inx = columns_id[key]\n",
    "                extra_info_vec[extra_info_inx-1] = 1\n",
    "        elif operator == 'Hash Join' or operator == 'Merge Join' or operator == 'Nested Loop':\n",
    "            condition1_vec = encode_condition(node['condition'], None, None, condition_max_num)\n",
    "        elif operator == 'Aggregate':\n",
    "            for key in node['group_keys']:\n",
    "                extra_info_inx = columns_id[key]\n",
    "                extra_info_vec[extra_info_inx-1] = 1\n",
    "        elif operator == 'Seq Scan' or operator == 'Bitmap Heap Scan' or operator == 'Index Scan' or operator == 'Bitmap Index Scan' or operator == 'Index Only Scan':\n",
    "            relation_name = node['relation_name']\n",
    "            index_name = node['index_name']\n",
    "            if relation_name != None:\n",
    "                extra_info_inx = tables_id[relation_name]\n",
    "            else:\n",
    "                extra_info_inx = indexes_id[index_name]\n",
    "            extra_info_vec[extra_info_inx-1] = 1\n",
    "            condition1_vec = encode_condition(node['condition_filter'], relation_name, index_name, condition_max_num)\n",
    "            condition2_vec = encode_condition(node['condition_index'], relation_name, index_name, condition_max_num)\n",
    "            if 'bitmap' in node:\n",
    "                ### Samples Starts\n",
    "                sample_vec = encode_sample(node['bitmap'])\n",
    "                ### Samples Ends\n",
    "                has_condition = 1\n",
    "            if 'bitmap_filter' in node:\n",
    "                ### Samples Starts\n",
    "                sample_vec = bitand(encode_sample(node['bitmap_filter']), sample_vec)\n",
    "                ### Samples Ends\n",
    "                has_condition = 1\n",
    "            if 'bitmap_index' in node:\n",
    "                ### Samples Starts\n",
    "                sample_vec = bitand(encode_sample(node['bitmap_index']), sample_vec)\n",
    "                ### Samples Ends\n",
    "                has_condition = 1\n",
    "\n",
    "#     print 'operator: ', operator_vec\n",
    "#     print 'extra_infos: ', extra_info_vec\n",
    "    return operator_vec, extra_info_vec, condition1_vec, condition2_vec, sample_vec, has_condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. Util Functions related to Tree Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.048355400Z",
     "start_time": "2023-08-30T13:12:05.002358Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define TreeNode class\n",
    "class TreeNode(object):\n",
    "    def __init__(self, current_vec, parent, idx, level_id):\n",
    "        self.item = current_vec\n",
    "        self.idx = idx\n",
    "        self.level_id = level_id\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "    def get_parent(self):\n",
    "        return self.parent\n",
    "    def get_item(self):\n",
    "        return self.item\n",
    "    def get_children(self):\n",
    "        return self.children\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "    def get_idx(self):\n",
    "        return self.idx\n",
    "    def __str__(self):\n",
    "        return 'level_id: ' + self.level_id + '; idx: ' + self.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.048355400Z",
     "start_time": "2023-08-30T13:12:05.020355500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate tree (consisted of TreeNode) from vector representation\n",
    "def recover_tree(vecs, parent, start_idx):\n",
    "    if len(vecs) == 0:\n",
    "        return vecs, start_idx\n",
    "    if vecs[0] == None:\n",
    "        return vecs[1:], start_idx+1\n",
    "    node = TreeNode(vecs[0], parent, start_idx, -1)\n",
    "    while True:\n",
    "        vecs, start_idx = recover_tree(vecs[1:], node, start_idx+1)\n",
    "        parent.add_child(node)\n",
    "        if len(vecs) == 0:\n",
    "            return vecs, start_idx\n",
    "        if vecs[0] == None:\n",
    "            return vecs[1:], start_idx+1\n",
    "        node = TreeNode(vecs[0], parent, start_idx, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.049356100Z",
     "start_time": "2023-08-30T13:12:05.031361200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# level limited DFS, append nodes to nodes_by_level\n",
    "def dfs_tree_to_level(root, level_id, nodes_by_level):\n",
    "    root.level_id = level_id\n",
    "    if len(nodes_by_level) <= level_id:\n",
    "        nodes_by_level.append([])\n",
    "    nodes_by_level[level_id].append(root)\n",
    "    root.idx = len(nodes_by_level[level_id])\n",
    "    for c in root.get_children():\n",
    "        dfs_tree_to_level(c, level_id+1, nodes_by_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.064354600Z",
     "start_time": "2023-08-30T13:12:05.048355400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# debug function to test level limited DFS\n",
    "def debug_nodes_by_level(nodes_by_level):\n",
    "    for nodes in nodes_by_level:\n",
    "        for node in nodes:\n",
    "            whitespace = ''\n",
    "            for i in range(node.level_id):\n",
    "                whitespace += ' '\n",
    "            print (whitespace + 'level_id: ' + str(node.level_id))\n",
    "            print (whitespace + 'idx: ' + str(node.idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Util Functions related to Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.098356Z",
     "start_time": "2023-08-30T13:12:05.067355100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate corresponding tree structure for an input plan\n",
    "def encode_plan_job(plan, condition_max_num):\n",
    "    operators, extra_infos, condition1s, condition2s, samples, condition_masks = [], [], [], [], [], []\n",
    "    mapping = []\n",
    "\n",
    "    nodes_by_level = []\n",
    "    node = TreeNode(plan[0], None, 0, -1)\n",
    "    recover_tree(plan[1:], node, 1)\n",
    "    dfs_tree_to_level(node, 0, nodes_by_level)\n",
    "\n",
    "#     print (plan)\n",
    "#     debug_nodes_by_level(nodes_by_level)\n",
    "\n",
    "\n",
    "    for level in nodes_by_level:\n",
    "        operators.append([])\n",
    "        extra_infos.append([])\n",
    "        condition1s.append([])\n",
    "        condition2s.append([])\n",
    "        samples.append([])\n",
    "        condition_masks.append([])\n",
    "        mapping.append([])\n",
    "        for node in level:\n",
    "            operator, extra_info, condition1, condition2, sample, condition_mask = encode_node_job(node.item, condition_max_num)\n",
    "            operators[-1].append(operator)\n",
    "            extra_infos[-1].append(extra_info)\n",
    "            condition1s[-1].append(condition1)\n",
    "            condition2s[-1].append(condition2)\n",
    "            samples[-1].append(sample)\n",
    "            condition_masks[-1].append(condition_mask)\n",
    "            if len(node.children) == 2:\n",
    "                mapping[-1].append([n.idx for n in node.children])\n",
    "            elif len(node.children) == 1:\n",
    "                mapping[-1].append([node.children[0].idx, 0])\n",
    "            else:\n",
    "                mapping[-1].append([0, 0])\n",
    "#     num_pad = plan_node_max_num - len(operators)\n",
    "#     masks = [0 for _ in range(plan_node_max_num)]\n",
    "#     for i in range(len(operators)):\n",
    "#         if operators[i].sum() > 0:\n",
    "#             masks[i] = 1\n",
    "#         else:\n",
    "#             masks[i] = 0\n",
    "#     masks = np.array(masks)\n",
    "#     condition_masks = np.array(condition_masks)\n",
    "#     operators, extra_infos, condition1s, condition2s = np.pad(operators, ((0, num_pad), (0,0)), 'constant'), np.pad(extra_infos, ((0, num_pad), (0,0)), 'constant'),np.pad(condition1s, ((0, num_pad), (0,0), (0,0)), 'constant'),np.pad(condition2s, ((0, num_pad), (0,0), (0,0)), 'constant')\n",
    "#     samples = np.pad(samples, ((0, num_pad), (0,0)), 'constant')\n",
    "#     condition_masks = np.pad(condition_masks, (0, num_pad), 'constant')\n",
    "    return operators, extra_infos, condition1s, condition2s, samples, condition_masks, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.099355900Z",
     "start_time": "2023-08-30T13:12:05.079356200Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize label\n",
    "def normalize_label(labels, mini, maxi):\n",
    "    labels_norm = (torch.log(labels) - mini) / (maxi - mini)\n",
    "    labels_norm = torch.min(labels_norm, torch.ones_like(labels_norm))\n",
    "    labels_norm = torch.max(labels_norm, torch.zeros_like(labels_norm))\n",
    "    return labels_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.123354800Z",
     "start_time": "2023-08-30T13:12:05.096360Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unnormalize values\n",
    "def unnormalize(vecs, mini, maxi):\n",
    "    return torch.exp(vecs * (maxi - mini) + mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.155395300Z",
     "start_time": "2023-08-30T13:12:05.109357200Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate plan_node_max_num, condition_max_num, cost_label_min, cost_label_max, card_label_min, card_label_max\n",
    "def obtain_upper_bound_query_size(path):\n",
    "    plan_node_max_num = 0\n",
    "    condition_max_num = 0\n",
    "    cost_label_max = 0.0\n",
    "    cost_label_min = 9999999999.0\n",
    "    card_label_max = 0.0\n",
    "    card_label_min = 9999999999.0\n",
    "    plans = []\n",
    "    with open(path, 'r') as f:\n",
    "        for plan in f.readlines():\n",
    "            plan = json.loads(plan)\n",
    "            plans.append(plan)\n",
    "            cost = plan['cost']\n",
    "            cardinality = plan['cardinality']\n",
    "            if cost > cost_label_max:\n",
    "                cost_label_max = cost\n",
    "            elif cost < cost_label_min:\n",
    "                cost_label_min = cost\n",
    "            if cardinality > card_label_max:\n",
    "                card_label_max = cardinality\n",
    "            elif cardinality < card_label_min:\n",
    "                card_label_min = cardinality\n",
    "            sequence = plan['seq']\n",
    "            plan_node_num = len(sequence)\n",
    "            if plan_node_num > plan_node_max_num:\n",
    "                plan_node_max_num = plan_node_num\n",
    "            for node in sequence:\n",
    "                if node == None:\n",
    "                    continue\n",
    "                if 'condition_filter' in node:\n",
    "                    condition_num = len(node['condition_filter'])\n",
    "                    if condition_num > condition_max_num:\n",
    "                        condition_max_num = condition_num\n",
    "                if 'condition_index' in node:\n",
    "                    condition_num = len(node['condition_index'])\n",
    "                    if condition_num > condition_max_num:\n",
    "                        condition_max_num = condition_num\n",
    "    cost_label_min, cost_label_max = math.log(cost_label_min), math.log(cost_label_max)\n",
    "    # lql: adpat to the min card is 0\n",
    "    if card_label_min <= 0:\n",
    "        card_label_min = 1\n",
    "    card_label_min, card_label_max = math.log(card_label_min), math.log(card_label_max)\n",
    "    \n",
    "    \n",
    "    print('plan_node_max_num: ', plan_node_max_num)\n",
    "    print('condition_max_num: ', condition_max_num)\n",
    "    print('cost_label_min: ', cost_label_min)\n",
    "    print('cost_label_max: ', cost_label_max)\n",
    "    print('card_label_min: ', card_label_min)\n",
    "    print('card_label_max: ', card_label_max)\n",
    "    return plan_node_max_num, condition_max_num, cost_label_min, cost_label_max, card_label_min, card_label_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.156356700Z",
     "start_time": "2023-08-30T13:12:05.128357100Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge plan level 2 into level1?\n",
    "def merge_plans_level(level1, level2, isMapping=False):\n",
    "    for idx, level in enumerate(level2):\n",
    "        if idx >= len(level1):\n",
    "            level1.append([])\n",
    "        if isMapping:\n",
    "            if idx < len(level1) - 1:\n",
    "                base = len(level1[idx+1])\n",
    "                for i in range(len(level)):\n",
    "                    if level[i][0] > 0:\n",
    "                        level[i][0] += base\n",
    "                    if level[i][1] > 0:\n",
    "                        level[i][1] += base\n",
    "        level1[idx] += level\n",
    "    return level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.156356700Z",
     "start_time": "2023-08-30T13:12:05.151360700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate training data from plans loaded from json file\n",
    "def make_data_job(plans):\n",
    "    target_cost_batch = []\n",
    "    target_card_batch = []\n",
    "    operators_batch = []\n",
    "    extra_infos_batch = []\n",
    "    condition1s_batch = []\n",
    "    condition2s_batch = []\n",
    "    node_masks_batch = []\n",
    "    samples_batch = []\n",
    "    condition_masks_batch = []\n",
    "    mapping_batch = []\n",
    "\n",
    "    # for plan in plans:\n",
    "    # lql: print idx for debug\n",
    "    for i in range(len(plans)):\n",
    "        print('idx: ', i)\n",
    "        plan = plans[i]\n",
    "        target_cost = plan['cost']\n",
    "        target_cardinality = plan['cardinality']\n",
    "        target_cost_batch.append(target_cost)\n",
    "        target_card_batch.append(target_cardinality)\n",
    "        plan = plan['seq']\n",
    "        operators, extra_infos, condition1s, condition2s, samples, condition_masks, mapping = encode_plan_job(plan, condition_max_num)\n",
    "\n",
    "        operators_batch = merge_plans_level(operators_batch, operators)\n",
    "        extra_infos_batch = merge_plans_level(extra_infos_batch, extra_infos)\n",
    "        condition1s_batch = merge_plans_level(condition1s_batch, condition1s)\n",
    "        condition2s_batch = merge_plans_level(condition2s_batch, condition2s)\n",
    "        samples_batch = merge_plans_level(samples_batch, samples)\n",
    "        condition_masks_batch = merge_plans_level(condition_masks_batch, condition_masks)\n",
    "        mapping_batch = merge_plans_level(mapping_batch, mapping, True)\n",
    "    max_nodes = 0\n",
    "    for o in operators_batch:\n",
    "        if len(o) > max_nodes:\n",
    "            max_nodes = len(o)\n",
    "    print (max_nodes)\n",
    "    print (len(condition2s_batch))\n",
    "    operators_batch = np.array([np.pad(v, ((0, max_nodes - len(v)),(0,0)), 'constant') for v in operators_batch])\n",
    "    extra_infos_batch = np.array([np.pad(v, ((0, max_nodes - len(v)),(0,0)), 'constant') for v in extra_infos_batch])\n",
    "    condition1s_batch = np.array([np.pad(v, ((0, max_nodes - len(v)),(0,0),(0,0)), 'constant') for v in condition1s_batch])\n",
    "    condition2s_batch = np.array([np.pad(v, ((0, max_nodes - len(v)),(0,0),(0,0)), 'constant') for v in condition2s_batch])\n",
    "    samples_batch = np.array([np.pad(v, ((0, max_nodes - len(v)),(0,0)), 'constant') for v in samples_batch])\n",
    "    condition_masks_batch = np.array([np.pad(v, (0, max_nodes - len(v)), 'constant') for v in condition_masks_batch])\n",
    "    mapping_batch = np.array([np.pad(v, ((0, max_nodes - len(v)),(0,0)), 'constant') for v in mapping_batch])\n",
    "\n",
    "    print ('operators_batch: ', operators_batch.shape)\n",
    "\n",
    "    target_cost_batch = torch.FloatTensor(target_cost_batch)\n",
    "    target_card_batch = torch.FloatTensor(target_card_batch)\n",
    "    operators_batch = torch.FloatTensor([operators_batch])\n",
    "    extra_infos_batch = torch.FloatTensor([extra_infos_batch])\n",
    "    condition1s_batch = torch.FloatTensor([condition1s_batch])\n",
    "    condition2s_batch = torch.FloatTensor([condition2s_batch])\n",
    "    samples_batch = torch.FloatTensor([samples_batch])\n",
    "    condition_masks_batch = torch.FloatTensor([condition_masks_batch])\n",
    "    mapping_batch = torch.FloatTensor([mapping_batch])\n",
    "\n",
    "    target_cost_batch = normalize_label(target_cost_batch, cost_label_min, cost_label_max)\n",
    "    target_card_batch = normalize_label(target_card_batch, card_label_min, card_label_max)\n",
    "\n",
    "    return (target_cost_batch, target_card_batch, operators_batch, extra_infos_batch, condition1s_batch, condition2s_batch, samples_batch, condition_masks_batch, mapping_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.174355500Z",
     "start_time": "2023-08-30T13:12:05.157356500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into chunks\n",
    "def chunks(arr, batch_size):\n",
    "    return [arr[i:i+batch_size] for i in range(0, len(arr), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.198387200Z",
     "start_time": "2023-08-30T13:12:05.176356900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save training data into npy files\n",
    "def save_data_job(plans, istest = False, batch_size=64, directory='/home/sunji/learnedcardinality/job'):\n",
    "    if istest:\n",
    "        suffix = 'test_'\n",
    "    else:\n",
    "        suffix = ''\n",
    "    batch_id = 0\n",
    "    for batch_id, plans_batch in enumerate(chunks(plans, batch_size)):\n",
    "        print ('batch_id', batch_id, len(plans_batch))\n",
    "        target_cost_batch, target_cardinality_batch, operators_batch, extra_infos_batch, condition1s_batch, condition2s_batch, samples_batch, condition_masks_batch, mapping_batch = make_data_job(plans_batch)\n",
    "        np.save(directory+'/target_cost_'+suffix+str(batch_id)+'.np', target_cost_batch.numpy())\n",
    "        np.save(directory+'/target_cardinality_'+suffix+str(batch_id)+'.np', target_cardinality_batch.numpy())\n",
    "        np.save(directory+'/operators_'+suffix+str(batch_id)+'.np', operators_batch.numpy())\n",
    "        np.save(directory+'/extra_infos_'+suffix+str(batch_id)+'.np', extra_infos_batch.numpy())\n",
    "        np.save(directory+'/condition1s_'+suffix+str(batch_id)+'.np', condition1s_batch.numpy())\n",
    "        np.save(directory+'/condition2s_'+suffix+str(batch_id)+'.np', condition2s_batch.numpy())\n",
    "        np.save(directory+'/samples_'+suffix+str(batch_id)+'.np', samples_batch.numpy())\n",
    "        np.save(directory+'/condition_masks_'+suffix+str(batch_id)+'.np', condition_masks_batch.numpy())\n",
    "        np.save(directory+'/mapping_'+suffix+str(batch_id)+'.np', mapping_batch.numpy())\n",
    "        print ('saved: ', str(batch_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.217355Z",
     "start_time": "2023-08-30T13:12:05.191356500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load training data from npy files\n",
    "def get_batch_job(batch_id, istest=False, directory='tlstm_files/job'):\n",
    "    if istest:\n",
    "        suffix = 'test_'\n",
    "    else:\n",
    "        suffix = ''\n",
    "    target_cost_batch = np.load(directory+'/target_cost_'+suffix+str(batch_id)+'.np.npy')\n",
    "    target_cardinality_batch = np.load(directory+'/target_cardinality_'+suffix+str(batch_id)+'.np.npy')\n",
    "    operators_batch = np.load(directory+'/operators_'+suffix+str(batch_id)+'.np.npy')\n",
    "    extra_infos_batch = np.load(directory+'/extra_infos_'+suffix+str(batch_id)+'.np.npy')\n",
    "    condition1s_batch = np.load(directory+'/condition1s_'+suffix+str(batch_id)+'.np.npy')\n",
    "    condition2s_batch = np.load(directory+'/condition2s_'+suffix+str(batch_id)+'.np.npy')\n",
    "    samples_batch = np.load(directory+'/samples_'+suffix+str(batch_id)+'.np.npy')\n",
    "    condition_masks_batch = np.load(directory+'/condition_masks_'+suffix+str(batch_id)+'.np.npy')\n",
    "    mapping_batch = np.load(directory+'/mapping_'+suffix+str(batch_id)+'.np.npy')\n",
    "    return target_cost_batch, target_cardinality_batch, operators_batch, extra_infos_batch, condition1s_batch, condition2s_batch, samples_batch, condition_masks_batch, mapping_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.227389800Z",
     "start_time": "2023-08-30T13:12:05.207357700Z"
    }
   },
   "outputs": [],
   "source": [
    "# store train data from json file in path into npy files\n",
    "def encode_train_plan_seq_save(path, batch_size=64, directory='tlstm_files/job'):\n",
    "    train_plans = []\n",
    "    with open(path, 'r') as f:\n",
    "        for idx, seq in enumerate(f.readlines()):\n",
    "            plan = json.loads(seq)\n",
    "            train_plans.append(plan)\n",
    "#     shuffle(test_plans)\n",
    "    save_data_job(plans=train_plans, batch_size=batch_size, directory=directory)\n",
    "# store test data from json file in path into npy files\n",
    "def encode_test_plan_seq_save(path, batch_size=64, directory='tlstm_files/job'):\n",
    "    test_plans = []\n",
    "    with open(path, 'r') as f:\n",
    "        for idx, seq in enumerate(f.readlines()):\n",
    "            plan = json.loads(seq)\n",
    "            test_plans.append(plan)\n",
    "#     shuffle(test_plans)\n",
    "    save_data_job(plans=test_plans, istest=True, batch_size=batch_size, directory=directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Util Functions related to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.312356300Z",
     "start_time": "2023-08-30T13:12:05.298355Z"
    }
   },
   "outputs": [],
   "source": [
    "class Representation(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, hid_dim, middle_result_dim, task_num):\n",
    "        super(Representation, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hid_dim)\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        \n",
    "        self.sample_mlp = nn.Linear(1000, hid_dim)\n",
    "        self.condition_mlp = nn.Linear(hidden_dim, hid_dim)\n",
    "#         self.out_mlp1 = nn.Linear(hidden_dim, middle_result_dim)\n",
    "#         self.hid_mlp1 = nn.Linear(15+108+2*hid_dim, hid_dim)\n",
    "#         self.out_mlp1 = nn.Linear(hid_dim, middle_result_dim)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(15+108+2*hid_dim, hidden_dim, batch_first=True)\n",
    "#         self.lstm2_binary = nn.LSTM(15+108+2*hid_dim, hidden_dim, batch_first=True)\n",
    "#         self.lstm2_binary = nn.LSTM(15+108+2*hid_dim, hidden_dim, batch_first=True)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_dim)\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hid_mlp2_task1 = nn.Linear(hidden_dim, hid_dim)\n",
    "        self.hid_mlp2_task2 = nn.Linear(hidden_dim, hid_dim)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(hid_dim)\n",
    "        self.hid_mlp3_task1 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.hid_mlp3_task2 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.out_mlp2_task1 = nn.Linear(hid_dim, 1)\n",
    "        self.out_mlp2_task2 = nn.Linear(hid_dim, 1)\n",
    "    #         self.hidden2values2 = nn.Linear(hidden_dim, action_num)\n",
    "\n",
    "    def init_hidden(self, hidden_dim, batch_size=1):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, batch_size, hidden_dim),\n",
    "                torch.zeros(1, batch_size, hidden_dim))\n",
    "    \n",
    "    def forward(self, operators, extra_infos, condition1s, condition2s, samples, condition_masks, mapping):\n",
    "        # condition1\n",
    "        batch_size = 0\n",
    "        for i in range(operators.size()[1]):\n",
    "            if operators[0][i].sum(0) != 0:\n",
    "                batch_size += 1\n",
    "            else:\n",
    "                break\n",
    "        print ('batch_size: ', batch_size)\n",
    "        \n",
    "#         print (operators.size())\n",
    "#         print (extra_infos.size())\n",
    "#         print (condition1s.size())\n",
    "#         print (condition2s.size())\n",
    "#         print (samples.size())\n",
    "#         print (condition_masks.size())\n",
    "#         print (mapping.size())\n",
    "        \n",
    "#         torch.Size([14, 133, 15])\n",
    "#         torch.Size([14, 133, 108])\n",
    "#         torch.Size([14, 133, 13, 1119])\n",
    "#         torch.Size([14, 133, 13, 1119])\n",
    "#         torch.Size([14, 133, 1000])\n",
    "#         torch.Size([14, 133, 1])\n",
    "#         torch.Size([14, 133, 2])\n",
    "        \n",
    "        num_level = condition1s.size()[0]\n",
    "        num_node_per_level = condition1s.size()[1]\n",
    "        num_condition_per_node = condition1s.size()[2]\n",
    "        condition_op_length = condition1s.size()[3]\n",
    "        \n",
    "        inputs = condition1s.view(num_level * num_node_per_level, num_condition_per_node, condition_op_length)\n",
    "        hidden = self.init_hidden(self.hidden_dim, num_level * num_node_per_level)\n",
    "        \n",
    "        out, hid = self.lstm1(inputs, hidden)\n",
    "        last_output1 = hid[0].view(num_level * num_node_per_level, -1)\n",
    "        \n",
    "        # condition2\n",
    "        num_level = condition2s.size()[0]\n",
    "        num_node_per_level = condition2s.size()[1]\n",
    "        num_condition_per_node = condition2s.size()[2]\n",
    "        condition_op_length = condition2s.size()[3]\n",
    "        \n",
    "        inputs = condition2s.view(num_level * num_node_per_level, num_condition_per_node, condition_op_length)\n",
    "        hidden = self.init_hidden(self.hidden_dim, num_level * num_node_per_level)\n",
    "        \n",
    "        out, hid = self.lstm1(inputs, hidden)\n",
    "        last_output2 = hid[0].view(num_level * num_node_per_level, -1)\n",
    "        \n",
    "        last_output1 = F.relu(self.condition_mlp(last_output1))\n",
    "        last_output2 = F.relu(self.condition_mlp(last_output2))\n",
    "        last_output = (last_output1 + last_output2) / 2\n",
    "        last_output = self.batch_norm1(last_output).view(num_level, num_node_per_level, -1)\n",
    "        \n",
    "#         print (last_output.size())\n",
    "#         torch.Size([14, 133, 256])\n",
    "        \n",
    "        sample_output = F.relu(self.sample_mlp(samples))\n",
    "        sample_output = sample_output * condition_masks\n",
    "\n",
    "        out = torch.cat((operators, extra_infos, last_output, sample_output), 2)\n",
    "#         print (out.size())\n",
    "#         torch.Size([14, 133, 635])\n",
    "#         out = out * node_masks\n",
    "        start = time.time()\n",
    "        hidden = self.init_hidden(self.hidden_dim, num_node_per_level)\n",
    "        last_level = out[num_level-1].view(num_node_per_level, 1, -1)\n",
    "#         torch.Size([133, 1, 635])\n",
    "        _, (hid, cid) = self.lstm2(last_level, hidden)\n",
    "        mapping = mapping.long()\n",
    "        for idx in reversed(range(0, num_level-1)):\n",
    "            mapp_left = mapping[idx][:,0]\n",
    "            mapp_right = mapping[idx][:,1]\n",
    "            pad = torch.zeros_like(hid)[:,0].unsqueeze(1)\n",
    "            next_hid = torch.cat((pad, hid), 1)\n",
    "            pad = torch.zeros_like(cid)[:,0].unsqueeze(1)\n",
    "            next_cid = torch.cat((pad, cid), 1)\n",
    "            hid_left = torch.index_select(next_hid, 1, mapp_left)\n",
    "            cid_left = torch.index_select(next_cid, 1, mapp_left)\n",
    "            hid_right = torch.index_select(next_hid, 1, mapp_right)\n",
    "            cid_right = torch.index_select(next_cid, 1, mapp_right)\n",
    "            hid = (hid_left + hid_right) / 2\n",
    "            cid = (cid_left + cid_right) / 2\n",
    "            last_level = out[idx].view(num_node_per_level, 1, -1)\n",
    "            _, (hid, cid) = self.lstm2(last_level, (hid, cid))\n",
    "        output = hid[0]\n",
    "#         print (output.size())\n",
    "#         torch.Size([133, 128])\n",
    "        end = time.time()\n",
    "        print ('Forest Evaluate Running Time: ', end - start)\n",
    "        last_output = output[0:batch_size]\n",
    "        out = self.batch_norm2(last_output)\n",
    "        \n",
    "        out_task1 = F.relu(self.hid_mlp2_task1(out))\n",
    "        out_task1 = self.batch_norm3(out_task1)\n",
    "        out_task1 = F.relu(self.hid_mlp3_task1(out_task1))\n",
    "        out_task1 = self.out_mlp2_task1(out_task1)\n",
    "        out_task1 = F.sigmoid(out_task1)\n",
    "        \n",
    "        out_task2 = F.relu(self.hid_mlp2_task2(out))\n",
    "        out_task2 = self.batch_norm3(out_task2)\n",
    "        out_task2 = F.relu(self.hid_mlp3_task2(out_task2))\n",
    "        out_task2 = self.out_mlp2_task2(out_task2)\n",
    "        out_task2 = F.sigmoid(out_task2)\n",
    "#         print 'out: ', out.size()\n",
    "        # batch_size * task_num\n",
    "        return out_task1, out_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.328355800Z",
     "start_time": "2023-08-30T13:12:05.313359300Z"
    }
   },
   "outputs": [],
   "source": [
    "# implement q-error loss function\n",
    "def qerror_loss(preds, targets, mini, maxi):\n",
    "    qerror = []\n",
    "    preds = unnormalize(preds, mini, maxi)\n",
    "    targets = unnormalize(targets, mini, maxi)\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i]/targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i]/preds[i])\n",
    "    return torch.mean(torch.cat(qerror)), torch.median(torch.cat(qerror)), torch.max(torch.cat(qerror)), torch.argmax(torch.cat(qerror))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:12:05.362354200Z",
     "start_time": "2023-08-30T13:12:05.341359700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(train_start, train_end, validate_start, validate_end, num_epochs, data_dir='tlstm_files/job'):\n",
    "    input_dim = condition_op_dim\n",
    "    hidden_dim = 128\n",
    "    hid_dim = 256\n",
    "    middle_result_dim = 128\n",
    "    task_num = 2\n",
    "    model = Representation(input_dim, hidden_dim, hid_dim, middle_result_dim, task_num)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        cost_loss_total = 0.\n",
    "        card_loss_total = 0.\n",
    "        model.train()\n",
    "        for batch_idx in range(train_start, train_end):\n",
    "            print ('batch_idx: ', batch_idx)\n",
    "            target_cost, target_cardinality, operatorss, extra_infoss, condition1ss, condition2ss, sampless, condition_maskss, mapping = get_batch_job(batch_idx, directory=data_dir)\n",
    "            target_cost, target_cardinality, operatorss, extra_infoss, condition1ss, condition2ss, sampless, condition_maskss, mapping = torch.FloatTensor(target_cost), torch.FloatTensor(target_cardinality),torch.FloatTensor(operatorss),torch.FloatTensor(extra_infoss),torch.FloatTensor(condition1ss),torch.FloatTensor(condition2ss), torch.FloatTensor(sampless), torch.FloatTensor(condition_maskss), torch.FloatTensor(mapping)\n",
    "            operatorss, extra_infoss, condition1ss, condition2ss, condition_maskss = operatorss.squeeze(0), extra_infoss.squeeze(0), condition1ss.squeeze(0), condition2ss.squeeze(0), condition_maskss.squeeze(0).unsqueeze(2)\n",
    "            sampless = sampless.squeeze(0)\n",
    "            mapping = mapping.squeeze(0)\n",
    "            target_cost, target_cardinality, operatorss, extra_infoss, condition1ss, condition2ss = Variable(target_cost), Variable(target_cardinality), Variable(operatorss), Variable(extra_infoss), Variable(condition1ss), Variable(condition2ss)\n",
    "            sampless = Variable(sampless)\n",
    "            optimizer.zero_grad()\n",
    "            estimate_cost,estimate_cardinality = model(operatorss, extra_infoss, condition1ss, condition2ss, sampless, condition_maskss, mapping)\n",
    "            target_cost = target_cost\n",
    "            target_cardinality = target_cardinality\n",
    "            cost_loss,cost_loss_median,cost_loss_max,cost_max_idx = qerror_loss(estimate_cost, target_cost, cost_label_min, cost_label_max)\n",
    "            card_loss,card_loss_median,card_loss_max,card_max_idx = qerror_loss(estimate_cardinality, target_cardinality, card_label_min, card_label_max)\n",
    "            print (card_loss.item(),card_loss_median.item(),card_loss_max.item(),card_max_idx.item())\n",
    "            loss = cost_loss + card_loss\n",
    "            cost_loss_total += cost_loss.item()\n",
    "            card_loss_total += card_loss.item()\n",
    "            start = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            end = time.time()\n",
    "            print ('batchward time: ',end - start)\n",
    "        batch_num = train_end - train_start\n",
    "        print(\"Epoch {}, training cost loss: {}, training card loss: {}\".format(epoch, cost_loss_total/batch_num, card_loss_total/batch_num))\n",
    "\n",
    "        cost_loss_total = 0.\n",
    "        card_loss_total = 0.\n",
    "        for batch_idx in range(validate_start, validate_end):\n",
    "            print ('batch_idx: ', batch_idx)\n",
    "            target_cost, target_cardinality, operatorss, extra_infoss, condition1ss, condition2ss, sampless, condition_maskss, mapping = get_batch_job(batch_idx, directory=data_dir)\n",
    "            target_cost, target_cardinality, operatorss, extra_infoss, condition1ss, condition2ss, sampless, condition_maskss, mapping = torch.FloatTensor(target_cost), torch.FloatTensor(target_cardinality),torch.FloatTensor(operatorss),torch.FloatTensor(extra_infoss),torch.FloatTensor(condition1ss),torch.FloatTensor(condition2ss), torch.FloatTensor(sampless), torch.FloatTensor(condition_maskss), torch.FloatTensor(mapping)\n",
    "            operatorss, extra_infoss, condition1ss, condition2ss, condition_maskss = operatorss.squeeze(0), extra_infoss.squeeze(0), condition1ss.squeeze(0), condition2ss.squeeze(0), condition_maskss.squeeze(0).unsqueeze(2)\n",
    "            sampless = sampless.squeeze(0)\n",
    "            mapping = mapping.squeeze(0)\n",
    "            target_cost, target_cardinality, operatorss, extra_infoss, condition1ss, condition2ss = Variable(target_cost), Variable(target_cardinality), Variable(operatorss), Variable(extra_infoss), Variable(condition1ss), Variable(condition2ss)\n",
    "            sampless = Variable(sampless)\n",
    "            estimate_cost,estimate_cardinality = model(operatorss, extra_infoss, condition1ss, condition2ss, sampless, condition_maskss, mapping)\n",
    "            target_cost = target_cost\n",
    "            target_cardinality = target_cardinality\n",
    "            cost_loss,cost_loss_median,cost_loss_max,cost_max_idx = qerror_loss(estimate_cost, target_cost, cost_label_min, cost_label_max)\n",
    "            card_loss,card_loss_median,card_loss_max,card_max_idx = qerror_loss(estimate_cardinality, target_cardinality, card_label_min, card_label_max)\n",
    "            print (card_loss.item(),card_loss_median.item(),card_loss_max.item(),card_max_idx.item())\n",
    "            loss = cost_loss + card_loss\n",
    "            cost_loss_total += cost_loss.item()\n",
    "            card_loss_total += card_loss.item()\n",
    "        batch_num = validate_end - validate_start\n",
    "        print(\"Epoch {}, validating cost loss: {}, validating card loss: {}\".format(epoch, cost_loss_total/batch_num, card_loss_total/batch_num))\n",
    "    end = time.time()\n",
    "    print (end-start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:26:09.496352100Z",
     "start_time": "2023-08-30T13:12:05.347357600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data prepared, use {} ms 184.53343653678894\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "data, indexes_id, tables_id, columns_id, physic_ops_id, compare_ops_id, bool_ops_id = prepare_dataset('tlstm_files/TPCH_10/tpch10_data_csv')\n",
    "print ('data prepared, use {} ms', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vectors loaded, use {} ms 43.259357213974\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "word_vectors = load_dictionary('tlstm_files/Initial_Dataset/wordvectors_updated.kv')\n",
    "print ('word_vectors loaded, use {} ms', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_max loaded, use {} ms 0.0\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "min_max_column = load_numeric_min_max('tlstm_files/TPCH_10/tpch_10_min_max_vals.json')\n",
    "print ('min_max loaded, use {} ms', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan_node_max_num:  44\n",
      "condition_max_num:  80\n",
      "cost_label_min:  5.676514047502008\n",
      "cost_label_max:  11.65474849922678\n",
      "card_label_min:  0.0\n",
      "card_label_max:  12.853563247110298\n",
      "plan loaded and obtain query upper size prepared, use {} ms 0.6880011558532715\n"
     ]
    }
   ],
   "source": [
    "index_total_num = len(indexes_id)\n",
    "table_total_num = len(tables_id)\n",
    "column_total_num = len(columns_id)\n",
    "physic_op_total_num = len(physic_ops_id)\n",
    "compare_ops_total_num = len(compare_ops_id)\n",
    "bool_ops_total_num = len(bool_ops_id)\n",
    "condition_op_dim = bool_ops_total_num + compare_ops_total_num+column_total_num+1000\n",
    "condition_op_dim_pro = bool_ops_total_num + column_total_num + 3\n",
    "t = time.time()\n",
    "plan_node_max_num, condition_max_num, cost_label_min, cost_label_max, card_label_min, card_label_max = obtain_upper_bound_query_size('tlstm_files/TPCH_10/plans_seq.json')\n",
    "print ('plan loaded and obtain query upper size prepared, use {} ms', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T13:26:09.555361100Z",
     "start_time": "2023-08-30T13:26:09.498352900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan_node_max_num:  44\n",
      "condition_max_num:  80\n",
      "cost_label_min:  5.676514047502008\n",
      "cost_label_max:  11.65474849922678\n",
      "card_label_min:  0.0\n",
      "card_label_max:  12.853563247110298\n"
     ]
    }
   ],
   "source": [
    "print('plan_node_max_num: ', plan_node_max_num)\n",
    "print('condition_max_num: ', condition_max_num)\n",
    "print('cost_label_min: ', cost_label_min)\n",
    "print('cost_label_max: ', cost_label_max)\n",
    "print('card_label_min: ', card_label_min)\n",
    "print('card_label_max: ', card_label_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id 0 64\n",
      "idx:  0\n",
      "lineitem.l_shipdate\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m encode_train_plan_seq_save(\u001b[39m'\u001b[39;49m\u001b[39mtlstm_files/TPCH_10/plans_seq.json\u001b[39;49m\u001b[39m'\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtlstm_files/TPCH_10/encoded_data\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mdata encoded, use \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m ms\u001b[39m\u001b[39m'\u001b[39m, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t)\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36mencode_train_plan_seq_save\u001b[1;34m(path, batch_size, directory)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             train_plans\u001b[39m.\u001b[39mappend(plan)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#     shuffle(test_plans)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     save_data_job(plans\u001b[39m=\u001b[39;49mtrain_plans, batch_size\u001b[39m=\u001b[39;49mbatch_size, directory\u001b[39m=\u001b[39;49mdirectory)\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36msave_data_job\u001b[1;34m(plans, istest, batch_size, directory)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_id, plans_batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(chunks(plans, batch_size)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mbatch_id\u001b[39m\u001b[39m'\u001b[39m, batch_id, \u001b[39mlen\u001b[39m(plans_batch))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     target_cost_batch, target_cardinality_batch, operators_batch, extra_infos_batch, condition1s_batch, condition2s_batch, samples_batch, condition_masks_batch, mapping_batch \u001b[39m=\u001b[39m make_data_job(plans_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     np\u001b[39m.\u001b[39msave(directory\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/target_cost_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39msuffix\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(batch_id)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.np\u001b[39m\u001b[39m'\u001b[39m, target_cost_batch\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     np\u001b[39m.\u001b[39msave(directory\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/target_cardinality_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39msuffix\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(batch_id)\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.np\u001b[39m\u001b[39m'\u001b[39m, target_cardinality_batch\u001b[39m.\u001b[39mnumpy())\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36mmake_data_job\u001b[1;34m(plans)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m target_card_batch\u001b[39m.\u001b[39mappend(target_cardinality)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m plan \u001b[39m=\u001b[39m plan[\u001b[39m'\u001b[39m\u001b[39mseq\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m operators, extra_infos, condition1s, condition2s, samples, condition_masks, mapping \u001b[39m=\u001b[39m encode_plan_job(plan, condition_max_num)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m operators_batch \u001b[39m=\u001b[39m merge_plans_level(operators_batch, operators)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m extra_infos_batch \u001b[39m=\u001b[39m merge_plans_level(extra_infos_batch, extra_infos)\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36mencode_plan_job\u001b[1;34m(plan, condition_max_num)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m mapping\u001b[39m.\u001b[39mappend([])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m level:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     operator, extra_info, condition1, condition2, sample, condition_mask \u001b[39m=\u001b[39m encode_node_job(node\u001b[39m.\u001b[39;49mitem, condition_max_num)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     operators[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mappend(operator)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     extra_infos[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mappend(extra_info)\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36mencode_node_job\u001b[1;34m(node, condition_max_num)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     extra_info_inx \u001b[39m=\u001b[39m indexes_id[index_name]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m extra_info_vec[extra_info_inx\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m condition1_vec \u001b[39m=\u001b[39m encode_condition(node[\u001b[39m'\u001b[39;49m\u001b[39mcondition_filter\u001b[39;49m\u001b[39m'\u001b[39;49m], relation_name, index_name, condition_max_num)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m condition2_vec \u001b[39m=\u001b[39m encode_condition(node[\u001b[39m'\u001b[39m\u001b[39mcondition_index\u001b[39m\u001b[39m'\u001b[39m], relation_name, index_name, condition_max_num)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mbitmap\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m node:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m### Samples Starts\u001b[39;00m\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36mencode_condition\u001b[1;34m(condition, relation_name, index_name, condition_max_num)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     vecs \u001b[39m=\u001b[39m [[\u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(condition_op_dim)]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     vecs \u001b[39m=\u001b[39m [encode_condition_op(condition_op, relation_name, index_name) \u001b[39mfor\u001b[39;00m condition_op \u001b[39min\u001b[39;00m condition]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m num_pad \u001b[39m=\u001b[39m condition_max_num \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(vecs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(vecs, ((\u001b[39m0\u001b[39m, num_pad),(\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m)), \u001b[39m'\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     vecs \u001b[39m=\u001b[39m [[\u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(condition_op_dim)]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     vecs \u001b[39m=\u001b[39m [encode_condition_op(condition_op, relation_name, index_name) \u001b[39mfor\u001b[39;00m condition_op \u001b[39min\u001b[39;00m condition]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m num_pad \u001b[39m=\u001b[39m condition_max_num \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(vecs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(vecs, ((\u001b[39m0\u001b[39m, num_pad),(\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m)), \u001b[39m'\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36mencode_condition_op\u001b[1;34m(condition_op, relation_name, index_name)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     operator_vec \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(compare_ops_total_num)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     operator_vec[operator_idx\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     right_value_vec \u001b[39m=\u001b[39m get_str_representation(right_value, left_value)\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m vec \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(bool_ops_total_num)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m vec \u001b[39m=\u001b[39m vec \u001b[39m+\u001b[39m left_value_vec \u001b[39m+\u001b[39m operator_vec \u001b[39m+\u001b[39m right_value_vec\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36mget_str_representation\u001b[1;34m(value, column)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m prefix \u001b[39m=\u001b[39m determine_prefix(column)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m value\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(v) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32md:\\DBGroup\\1.Learning_based_Optimizer\\Learning-based-cost-estimator\\End2EndCostEstimator_tpch.ipynb  42\u001b[0m in \u001b[0;36mdetermine_prefix\u001b[1;34m(column)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m     \u001b[39mprint\u001b[39m (column)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/DBGroup/1.Learning_based_Optimizer/Learning-based-cost-estimator/End2EndCostEstimator_tpch.ipynb#X53sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "encode_train_plan_seq_save('tlstm_files/TPCH_10/plans_seq.json', batch_size=64, directory='tlstm_files/TPCH_10/encoded_data')\n",
    "print ('data encoded, use {} ms', time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['lineitem'].dtypes['l_orderdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T15:17:02.843078100Z",
     "start_time": "2023-08-31T07:27:16.598780Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = train(0, 800, 800, 900, 20, data_dir='tlstm_files/job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-31T15:17:02.854077Z",
     "start_time": "2023-08-31T15:17:02.734078400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tlstm_model', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
